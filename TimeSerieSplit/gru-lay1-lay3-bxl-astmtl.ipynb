{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## General Import","metadata":{"id":"YIBknzfcshPu"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport gc\nimport time\nimport seaborn as sns; sns.set()\n\nimport os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n#pd.options.display.mpl_style = 'default'\nfrom datetime import datetime\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nfrom dateutil import tz\nimport geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom geopandas.tools import sjoin\n\nimport matplotlib.cm as cm\n\nimport folium\nimport shapely.geometry\n\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm\nimport matplotlib.colors as colors\n%matplotlib inline\n\nimport networkx as nx\nimport pickle\n\nimport osmnx as ox\nox.config(log_console=True, use_cache=True)\nox.__version__\n\nimport matplotlib.colors as mcolors\n","metadata":{"executionInfo":{"elapsed":3939,"status":"ok","timestamp":1615715731815,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"m7nfSj_GshPw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spektral ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom spektral.layers import GCNConv,  DiffusionConv, GATConv, GCSConv, GeneralConv, GlobalAvgPool, ChebConv\nfrom spektral.utils import gcn_filter\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers","metadata":{"executionInfo":{"elapsed":3934,"status":"ok","timestamp":1615715731816,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"b275b4IDshPx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","metadata":{"executionInfo":{"elapsed":3928,"status":"ok","timestamp":1615715731817,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"4nPI1eFgo16J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_graph = '../input/graph-results/graph_adj_bxl.pkl'\n\npath_feat_flow = '../input/graph-results/FEATURES_flow_bxl.csv'\npath_feat_vel = '../input/graph-results/FEATURES_vel_bxl.csv'\n\nwith open(path_graph,'rb') as f:\n    graph, adj_matrix, edges, G = pickle.load(f)\n\nadj_mx = nx.to_numpy_matrix(graph)\n\n# flow\nfeatures_flow = pd.read_csv(path_feat_flow).iloc[:,1:].values\n# vel\nfeatures_vel = pd.read_csv(path_feat_vel).iloc[:,1:].values","metadata":{"executionInfo":{"elapsed":19912,"status":"ok","timestamp":1615715747804,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"hmokpg02shPy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PARAMETERS","metadata":{"executionInfo":{"elapsed":19896,"status":"ok","timestamp":1615715747816,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"ZHtO7pnnxPQG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = 12\ngranularity = 2*2","metadata":{"executionInfo":{"elapsed":19893,"status":"ok","timestamp":1615715747817,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"sduHFHSexOTP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## split train/val/test","metadata":{"id":"8UvuJcrvshP0"}},{"cell_type":"code","source":"data_flow = features_flow[:, :-1]\ndata_vel = features_vel[:, :-1]\n\ndata = np.concatenate([data_flow, data_vel], axis=1)","metadata":{"executionInfo":{"elapsed":20179,"status":"ok","timestamp":1615715748106,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"nd6kl3lOmOth","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Covariates","metadata":{"executionInfo":{"elapsed":20171,"status":"ok","timestamp":1615715748110,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"BcOJcaqSvih_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"week = 168*2*2\n# week_test = 168*2\n\ndata_training = data[:-168*2*2*2, :]\n\nlist_training_validation = []\n\n# Train/test split 0\ndata_tr_0, data_val_0 = data_training[:-672, :], data_training[-672:-448, :]\nlist_training_validation.append([data_tr_0, data_val_0])\n\n# Train/test split 1\ndata_tr_1, data_val_1 = data_training[224:-448, :], data_training[-448:-224, :]\nlist_training_validation.append([data_tr_1, data_val_1])\n\n# Train/test split 2\ndata_tr_2, data_val_2 = data_training[448:-224, :], data_training[-224:, :]\nlist_training_validation.append([data_tr_2, data_val_2])\n\n\ndatetime = features_flow[:, -1]\n\nDATETIME = pd.DataFrame(datetime, columns=['Datetime'])\nDATETIME['Datetime'] = pd.to_datetime(DATETIME['Datetime'])\n\nDATETIME['minutes'] = DATETIME['Datetime'].dt.minute\nDATETIME['hour'] = DATETIME['Datetime'].dt.hour\n\nDATETIME['hour_x']=np.sin(DATETIME.hour*(2.*np.pi/23))\nDATETIME['hour_y']=np.cos(DATETIME.hour*(2.*np.pi/23))\n\nDATETIME['day'] = DATETIME['Datetime'].dt.day\nDATETIME['DayOfWeek'] = DATETIME['Datetime'].dt.dayofweek\n\nDATETIME['WorkingDays'] = DATETIME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATETIME = DATETIME.drop(['Datetime','minutes','hour','day'], axis=1).values\n\n# temporal features = 4\nfeat_time = 4\n\n\nlist_time = []\n\n\nDATETIME_training = DATETIME[:-168*2*2*2, :]\n\n# datetime Train/test split 0\ntime_tr_0, time_val_0 = DATETIME_training[:-672, :], DATETIME_training[-672:-448, :]\nlist_time.append([time_tr_0, time_val_0 ])\n\n# datetime Train/test split 1\ntime_tr_1, time_val_1 = DATETIME_training[224:-448, :], DATETIME_training[-448:-224, :]\nlist_time.append([time_tr_1, time_val_1 ])\n\n# datetime Train/test split 2\ntime_tr_2, time_val_2 = DATETIME_training[448:-224, :], DATETIME_training[-224:, :]\nlist_time.append([time_tr_2, time_val_2 ])","metadata":{"executionInfo":{"elapsed":20169,"status":"ok","timestamp":1615715748111,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"5GB9RXbYsoga","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nl2_reg = 5e-4  # Regularization rate for l2\n\n# Build model\nclass GCN_Net(Model):\n    \n    def __init__(self, state_size):\n        \n        super(GCN_Net, self).__init__()\n\n        self.street = 4524\n        \n        self.head = 4\n        \n        self.state_size = 64\n        \n        self.state_size_gru = state_size\n\n\n        # GCN 1st order approximation\n        self.gcn_flow_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n        self.gcn_flow_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n\n        self.gcn_vel_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n        self.gcn_vel_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n\n\n\n###################\n\n        self.lstm_flow_init = tf.compat.v1.keras.layers.CuDNNGRU(self.state_size_gru, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n\n\n        self.lstm_flow_fin = tf.compat.v1.keras.layers.CuDNNGRU(self.state_size_gru, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros')\n\n\n\n        self.drop_flow = tf.keras.layers.Dropout(0.1)\n\n        self.dense_flow_fin = tf.keras.layers.Dense(self.street*12,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n        \n\n##########################\n\n\n        self.lstm_vel_init = tf.compat.v1.keras.layers.CuDNNGRU(self.state_size_gru, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n\n\n\n        self.lstm_vel_fin = tf.compat.v1.keras.layers.CuDNNGRU(self.state_size_gru, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros')\n\n\n\n        self.drop_vel = tf.keras.layers.Dropout(0.1)\n\n\n        self.dense_vel_fin = tf.keras.layers.Dense(self.street*12,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n           \n##################################\n\n        self.attention_flow = tf.keras.layers.MultiHeadAttention(num_heads=self.head, key_dim=self.head)\n        self.attention_vel = tf.keras.layers.MultiHeadAttention(num_heads=self.head, key_dim=self.head)\n\n\n        self.dense_temporal_0 = tf.keras.layers.Dense(self.state_size_gru, activation ='relu') #tf.keras.layers.TimeDistributed()\n        self.dense_temporal_1 = tf.keras.layers.Dense(self.state_size, activation ='relu')\n        self.dense_temporal_2 = tf.keras.layers.Dense(self.state_size_gru)\n\n\n        self.drop = tf.keras.layers.Dropout(0.1)   \n\n        self.add = tf.keras.layers.Add()\n        self.norm = tf.keras.layers.LayerNormalization()\n\n        self.reshape = tf.keras.layers.Reshape([12, self.street])\n        self.repeat = tf.keras.layers.RepeatVector(12)\n        \n\n    def call(self, flow_vel, a, past_cov, fut_cov):\n\n        # sparse matrix\n        sparse_a = tf.sparse.from_dense(a)\n  \n        # flow\n        flow_0 = flow_vel[:, :, :-self.street]  \n        # velocity\n        vel_0  = flow_vel[:, :, -self.street:]\n        \n\n        # shape for gcn\n        flow_sh = tf.reshape(flow_0 , [flow_0.shape[0], flow_0 .shape[2], flow_0.shape[1]])\n        vel_sh = tf.reshape(vel_0 , [vel_0.shape[0], vel_0.shape[2], vel_0.shape[1]])\n\n        # two gcn on flow and vel\n        gcn_flow = self.gcn_flow_0_enc([flow_sh, sparse_a])\n        gcn_flow = self.gcn_flow_1_enc([gcn_flow, sparse_a])\n\n        gcn_vel = self.gcn_vel_0_enc([vel_sh, sparse_a])\n        gcn_vel = self.gcn_vel_1_enc([gcn_vel, sparse_a])\n\n        # shape for lstm\n        flow_sh = tf.reshape(gcn_flow, [gcn_flow.shape[0], gcn_flow.shape[2], gcn_flow.shape[1]])\n        vel_sh = tf.reshape(gcn_vel, [gcn_vel.shape[0], gcn_vel.shape[2], gcn_vel.shape[1]])\n\n        # two lstm models\n        flow_init, h_flow = self.lstm_flow_init(flow_sh)\n        vel_init, h_vel = self.lstm_vel_init(vel_sh)\n\n        # merge layer\n        # output\n        flow_vel_i = tf.concat([flow_init, vel_init], axis=2) # \n\n        flow_vel = self.dense_temporal_0(flow_vel_i)\n        flow_vel = self.dense_temporal_1(flow_vel)\n        flow_vel_f = self.dense_temporal_2(flow_vel)\n\n        # multi-head attention\n        # # Q: flow & vel\n        # # K: flow\n        att_flow, weight_flow = self.attention_flow(flow_vel_f, flow_init,\n                               return_attention_scores=True)\n\n\n        # # Q: vel & vel\n        # # K: vel\n        att_vel, weight_vel = self.attention_vel(flow_vel_f, vel_init,\n                               return_attention_scores=True)\n\n# -----\n\n        # concatenate covariates\n        # output flow_vel - covariates\n        fut_cov = tf.cast(fut_cov, dtype=tf.float32)\n        concat_flow = tf.concat([att_flow, fut_cov], axis=2)\n        concat_vel = tf.concat([att_vel, fut_cov], axis=2)\n  \n        # two models for flow and speed respectively\n\n        # flow\n        flow_final = self.lstm_flow_fin(concat_flow , initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) #  flow_vel_f, h_flow flow_vel_f[:,-1,:]\n        flow = self.drop_flow(flow_final)\n        flow = self.dense_flow_fin(flow)\n        flow = self.reshape(flow)\n\n\n        # velocity\n        vel_final = self.lstm_vel_fin(concat_vel, initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) \n        vel = self.drop_vel(vel_final)\n        vel = self.dense_vel_fin(vel)\n        vel = self.reshape(vel)\n\n\n        # concatenate two finals results\n        final = tf.concat([flow, vel], axis=-1)\n\n        return final\n\n\n\nloss_fn = tf.keras.losses.MeanAbsoluteError()","metadata":{"executionInfo":{"elapsed":22937,"status":"ok","timestamp":1615715750891,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"oYHppaM9shP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adj_matrix = gcn_filter(adj_matrix, symmetric=True)","metadata":{"executionInfo":{"elapsed":31896,"status":"ok","timestamp":1615715759853,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"v4iRDYOyi6FE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del data_tr, data_te, data_flow, data_vel, data, scaled_tr, scaled_te\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data_DL(INPUT, FEAT, BATCH):\n    \n    dataset = FEAT.reshape(FEAT.shape[0], FEAT.shape[1]) \n    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n    \n    inputs = dataset.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n    inputs = inputs.flat_map(lambda window: window.batch(INPUT))\n\n    targets = dataset.window(INPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    targets = targets.flat_map(lambda window: window.batch(INPUT))\n\n    dataset = tf.data.Dataset.zip((inputs, targets))\n    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport gc\n\nstart = time.time()\n\nlist_results = []\n\nstate_size_lst = [50, 150, 250]\n\nepochs = 10\n\nfor state_size in state_size_lst:\n\n    print('')\n    print('state_size: '+str(state_size))\n    print('EPOCHS: '+str(epochs))\n    print('')\n        \n    list_avg_loss_split = []\n    \n    timeseriesplit = zip(list_training_validation, list_time)\n    \n    optimizer = Adam(lr=0.001)\n    \n    # Training function\n    @tf.function\n    def train_on_batch(inputs, target, past_cov, cov):\n        loss = 0\n        with tf.GradientTape() as tape:\n            predictions = model(inputs, adj_matrix, past_cov, cov,  training=True)\n            loss = loss_fn(target, predictions)\n        variables = model.trainable_variables \n        gradients = tape.gradient(loss, variables)\n        optimizer.apply_gradients(zip(gradients, variables))\n        return loss\n    \n    # Create model\n    model = GCN_Net(state_size)        \n    \n    for data, time in timeseriesplit:\n        \n        data_tr, data_val = data[0], data[1]\n        time_tr, time_val = time[0], time[1]\n\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        scaler_cov = MinMaxScaler(feature_range=(0, 1))\n\n        # fit and transform\n        scaled_tr = scaler.fit_transform(data_tr)\n        # transform\n        scaled_val= scaler.transform(data_val)\n\n        # fit and transform\n        scaled_tr_cov = scaler_cov.fit_transform(time_tr)\n        # transform\n        scaled_val_cov = scaler_cov.transform(time_val)\n        \n        batch_train = 64 \n        batch_val = 64\n\n        # features\n        loader_tr = prepare_data_DL(12, scaled_tr, batch_train)\n        loader_val = prepare_data_DL(12, scaled_val, batch_val)\n\n        # covariates\n        loader_tr_cov = prepare_data_DL(12, scaled_tr_cov, batch_train)\n        loader_val_cov = prepare_data_DL(12, scaled_val_cov, batch_val)\n\n        # Keep results for plotting\n        train_loss_results = []\n        val_loss_results = []\n\n        samples_cov = list(loader_tr_cov)\n        samples_cov_val = list(loader_val_cov)\n\n        for epoch in range(epochs):\n\n            ## training\n            step = 0\n            epoch_loss_avg = tf.keras.metrics.Mean()\n            \n            for batch in loader_tr:\n\n                cov = samples_cov[step]\n                past_cov = cov[0]\n                fut_cov = cov[1]\n\n                # Training step\n                inputs, target = batch\n                loss = train_on_batch(inputs, target, past_cov, fut_cov)\n                # Track progress\n                epoch_loss_avg.update_state(loss)\n                step += 1\n\n        ## validation \n        step_val = 0\n        epoch_loss_avg_val = tf.keras.metrics.Mean()\n\n        for batch in loader_val:\n\n            cov = samples_cov_val[step_val]\n            past_cov = cov[0]\n            fut_cov = cov[1]\n\n            # Validation step\n            inputs, targ = batch\n            pred = model(inputs, adj_matrix, past_cov, fut_cov,  training=False)\n            loss = loss_fn(targ, pred)\n            epoch_loss_avg_val.update_state(loss)\n            step_val += 1\n\n        list_avg_loss_split.append(epoch_loss_avg_val.result())\n        print('loss per split: '+str(epoch_loss_avg_val.result()))\n\n        del inputs, target,  past_cov, fut_cov\n        gc.collect()\n    \n    print('')\n    print('average loss per 3 split: '+str(np.mean(list_avg_loss_split)))\n    list_results.append(np.mean(list_avg_loss_split))\n    epochs = epochs + 10\n    \n     ","metadata":{"executionInfo":{"elapsed":181346,"status":"ok","timestamp":1615716832009,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"mDbqw5X4shP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_results_array = [np.array(x) for x in list_results]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_results_avg = [np.mean(x) for x in list_results_array]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Saving the objects:\nwith open('results_GRU_bxl.pkl', 'wb') as f: \n    pickle.dump([list_results_array, list_results_avg], f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}