{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# drive mount","metadata":{"executionInfo":{"elapsed":475,"status":"ok","timestamp":1615724368426,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"7EU9ha7pM1iQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install geojson geopandas osmnx spektral matplotlib==3.1.3\n\n# from google.colab import files\n# from google.colab import drive\n\n# drive.mount('/content/drive')","metadata":{"executionInfo":{"elapsed":982,"status":"ok","timestamp":1615724368940,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"YkNeBz0Cxbwt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general import","metadata":{"executionInfo":{"elapsed":981,"status":"ok","timestamp":1615724368944,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"DqN83EQxMu0G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spektral ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport gc\nimport time\nimport seaborn as sns; sns.set()\n\nimport os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n#pd.options.display.mpl_style = 'default'\nfrom datetime import datetime\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nfrom dateutil import tz\nimport geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom geopandas.tools import sjoin\n\nimport matplotlib.cm as cm\n\nimport folium\nimport shapely.geometry\n\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm\nimport matplotlib.colors as colors\n%matplotlib inline\n\nimport networkx as nx\nimport pickle\n\nimport osmnx as ox\nox.config(log_console=True, use_cache=True)\nox.__version__\n\nimport matplotlib.colors as mcolors\nimport gc","metadata":{"executionInfo":{"elapsed":3535,"status":"ok","timestamp":1615724371501,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"m7nfSj_GshPw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom spektral.layers import GCNConv, ChebConv, GlobalSumPool, GlobalAvgPool #(channels, K=1)\n\n\nfrom spektral.utils import gcn_filter\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers","metadata":{"executionInfo":{"elapsed":3532,"status":"ok","timestamp":1615724371502,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"b275b4IDshPx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for reproducibility","metadata":{"executionInfo":{"elapsed":3529,"status":"ok","timestamp":1615724371502,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"z-HehR9JM6Dt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","metadata":{"executionInfo":{"elapsed":3526,"status":"ok","timestamp":1615724371504,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"BhWfC-IG3Tkq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imporing files","metadata":{"executionInfo":{"elapsed":3523,"status":"ok","timestamp":1615724371504,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"KshcMDinM__8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_graph = '../input/graph-results/graph_adj_bxl.pkl'\n\npath_feat_flow = '../input/graph-results/FEATURES_flow_bxl.csv'\npath_feat_vel = '../input/graph-results/FEATURES_vel_bxl.csv'\n\nwith open(path_graph,'rb') as f:\n    graph, adj_matrix, edges, G = pickle.load(f)\n\nadj_mx = nx.to_numpy_matrix(graph)\n\n# flow\nfeatures_flow = pd.read_csv(path_feat_flow).iloc[:,1:].values\n# vel\nfeatures_vel = pd.read_csv(path_feat_vel).iloc[:,1:].values","metadata":{"executionInfo":{"elapsed":20113,"status":"ok","timestamp":1615724388097,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"hmokpg02shPy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PARAMETERS","metadata":{"executionInfo":{"elapsed":20112,"status":"ok","timestamp":1615724388099,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"ZHtO7pnnxPQG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = 12\ngranularity = 2*2\n\nbatch_train = 64 # best 32\nbatch_test = 1","metadata":{"executionInfo":{"elapsed":20112,"status":"ok","timestamp":1615724388102,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"sduHFHSexOTP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## split train/val/test","metadata":{"id":"8UvuJcrvshP0"}},{"cell_type":"code","source":"data_flow = features_flow[:, :-1]\ndata_vel = features_vel[:, :-1]\n\ndata = np.concatenate([data_flow, data_vel], axis=1)","metadata":{"executionInfo":{"elapsed":20304,"status":"ok","timestamp":1615724388297,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"nd6kl3lOmOth","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/test split\ndata_tr, data_te = data[:-168*2*granularity, :], data[-168*2*granularity:, :]","metadata":{"executionInfo":{"elapsed":20303,"status":"ok","timestamp":1615724388298,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"GqWVkLFlshP0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time-based Covariates","metadata":{"executionInfo":{"elapsed":20301,"status":"ok","timestamp":1615724388299,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"BcOJcaqSvih_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datetime = features_flow[:, -1]\n\n# for figures\nprint_datetime = datetime[-168*2*granularity+inputs:]\n\nDATETIME = pd.DataFrame(datetime, columns=['Datetime'])\nDATETIME['Datetime'] = pd.to_datetime(DATETIME['Datetime'])\n\nDATETIME['minutes'] = DATETIME['Datetime'].dt.minute\nDATETIME['hour'] = DATETIME['Datetime'].dt.hour\n\nDATETIME['hour_x']=np.sin(DATETIME.hour*(2.*np.pi/23))\nDATETIME['hour_y']=np.cos(DATETIME.hour*(2.*np.pi/23))\n\nDATETIME['day'] = DATETIME['Datetime'].dt.day\nDATETIME['DayOfWeek'] = DATETIME['Datetime'].dt.dayofweek\n\nDATETIME['WorkingDays'] = DATETIME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATETIME = DATETIME.drop(['Datetime','minutes','hour','day'], axis=1).values\n\n# temporal features = 4\nfeat_time = 4\n\n# datetime Train/test split\ntime_tr, time_te = DATETIME[:-168*2*granularity, :], DATETIME[-168*2*granularity:, :]\n\n","metadata":{"executionInfo":{"elapsed":20297,"status":"ok","timestamp":1615724388299,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"5GB9RXbYsoga","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## scale data","metadata":{"id":"rnkgB_FYshP0"}},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaler_cov = MinMaxScaler(feature_range=(0, 1))\n\n# fit and transform\nscaled_tr = scaler.fit_transform(data_tr)\n# transform\nscaled_te = scaler.transform(data_te)\n\n# fit and transform\nscaled_tr_cov = scaler_cov.fit_transform(time_tr)\n# transform\nscaled_te_cov = scaler_cov.transform(time_te)","metadata":{"executionInfo":{"elapsed":22331,"status":"ok","timestamp":1615724390340,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"RjIvQ0QgshP1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prepare data for deep learning","metadata":{"id":"nsmXb48sshP1"}},{"cell_type":"code","source":"def prepare_data_DL(INPUT, FEAT, BATCH):\n    \n    dataset = FEAT.reshape(FEAT.shape[0], FEAT.shape[1]) \n    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n\n    inputs = dataset.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n    inputs = inputs.flat_map(lambda window: window.batch(INPUT))\n\n    targets = dataset.window(INPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    targets = targets.flat_map(lambda window: window.batch(INPUT))\n\n    dataset = tf.data.Dataset.zip((inputs, targets))\n    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n\n\n    return dataset","metadata":{"executionInfo":{"elapsed":22333,"status":"ok","timestamp":1615724390344,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"sQiniFq0shP2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features\nloader_tr = prepare_data_DL(inputs, scaled_tr, batch_train)\nloader_te = prepare_data_DL(inputs, scaled_te, batch_test)\n\n# covariates\nloader_tr_cov = prepare_data_DL(inputs, scaled_tr_cov, batch_train)\nloader_te_cov = prepare_data_DL(inputs, scaled_te_cov, batch_test)","metadata":{"executionInfo":{"elapsed":23554,"status":"ok","timestamp":1615724391568,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"FIs0wUItshP2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2_reg = 5e-4  # Regularization rate for l2\n\n# Build model\nclass GCN_Net(Model):\n    \n    def __init__(self, **kwargs):\n        \n        super().__init__(**kwargs)\n\n        self.street = 4524\n\n\n        # GCN 1st order approximation\n        self.gcn_flow_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n        self.gcn_flow_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n\n        self.gcn_vel_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n        self.gcn_vel_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n\n\n\n###################\n\n        # encoder GRU FLOW\n        # self.lstm_cells_flow_init = tf.keras.layers.GRUCell(150, activation ='relu',\n        #                                 kernel_initializer='glorot_uniform',\n        #                                 recurrent_initializer='glorot_uniform',\n        #                                 kernel_regularizer=regularizers.l2(0.001),\n        #                                 bias_initializer='zeros', dropout=0.0) \n        \n        # self.lstm_flow_init = tf.keras.layers.RNN(self.lstm_cells_flow_init, return_sequences = True, return_state =True)\n\n        self.lstm_flow_init = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n\n        # decoder GRU FLOW\n        # self.lstm_cells_flow_fin = tf.keras.layers.GRUCell(150, activation ='relu',\n        #                                 kernel_initializer='glorot_uniform',\n        #                                 recurrent_initializer='glorot_uniform',\n        #                                 kernel_regularizer=regularizers.l2(0.001),\n        #                                 bias_initializer='zeros', dropout=0.0) \n        \n        # self.lstm_flow_fin = tf.keras.layers.RNN(self.lstm_cells_flow_fin)\n\n        self.lstm_flow_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros')\n\n\n\n        self.drop_flow = tf.keras.layers.Dropout(0.1)\n\n        self.dense_flow_fin = tf.keras.layers.Dense(self.street*12,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n        \n\n##########################\n\n        # encoder GRU VEL\n        # self.lstm_cells_vel_init = tf.keras.layers.GRUCell(150, activation ='relu',\n        #                                 kernel_initializer='glorot_uniform',\n        #                                 recurrent_initializer='glorot_uniform',\n        #                                 kernel_regularizer=regularizers.l2(0.001),\n        #                                 bias_initializer='zeros', dropout=0.0) \n        \n        # self.lstm_vel_init = tf.keras.layers.RNN(self.lstm_cells_vel_init, return_sequences = True, return_state =True)\n\n        self.lstm_vel_init = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n\n\n        # decoder GRU VEL\n        # self.lstm_cells_vel_fin = tf.keras.layers.GRUCell(150, activation ='relu',\n        #                                 kernel_initializer='glorot_uniform',\n        #                                 recurrent_initializer='glorot_uniform',\n        #                                 kernel_regularizer=regularizers.l2(0.001),\n        #                                 bias_initializer='zeros', dropout=0.0) \n        \n        # self.lstm_vel_fin = tf.keras.layers.RNN(self.lstm_cells_vel_fin)\n\n\n        self.lstm_vel_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros')\n\n\n\n        self.drop_vel = tf.keras.layers.Dropout(0.1)\n\n\n        self.dense_vel_fin = tf.keras.layers.Dense(self.street*12,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n        \n\n        \n##################################\n\n        self.attention_flow = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n        self.attention_vel = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n\n\n        self.dense_temporal_0 = tf.keras.layers.Dense(150, activation ='relu') #tf.keras.layers.TimeDistributed()\n        self.dense_temporal_1 = tf.keras.layers.Dense(64, activation ='relu')\n        self.dense_temporal_2 = tf.keras.layers.Dense(150)\n\n\n        self.drop = tf.keras.layers.Dropout(0.1)   \n\n        self.add = tf.keras.layers.Add()\n        self.norm = tf.keras.layers.LayerNormalization()\n\n        self.reshape = tf.keras.layers.Reshape([12, self.street])\n        self.repeat = tf.keras.layers.RepeatVector(12)\n        \n\n    def call(self, flow_vel, a, past_cov, fut_cov):\n\n        # sparse matrix\n        sparse_a = tf.sparse.from_dense(a)\n  \n        # flow\n        flow_0 = flow_vel[:, :, :-self.street] \n        # velocity\n        vel_0  = flow_vel[:, :, -self.street:]\n        \n\n        # shape for gcn\n        flow_sh = tf.reshape(flow_0 , [flow_0.shape[0], flow_0 .shape[2], flow_0.shape[1]])\n        vel_sh = tf.reshape(vel_0 , [vel_0.shape[0], vel_0.shape[2], vel_0.shape[1]])\n\n        # two gcn on flow and vel\n        gcn_flow = self.gcn_flow_0_enc([flow_sh, sparse_a])\n        gcn_flow = self.gcn_flow_1_enc([gcn_flow, sparse_a])\n\n        gcn_vel = self.gcn_vel_0_enc([vel_sh, sparse_a])\n        gcn_vel = self.gcn_vel_1_enc([gcn_vel, sparse_a])\n        \n        # shape for lstm\n        flow_sh = tf.reshape(gcn_flow, [gcn_flow.shape[0], gcn_flow.shape[2], gcn_flow.shape[1]])\n        vel_sh = tf.reshape(gcn_vel, [gcn_vel.shape[0], gcn_vel.shape[2], gcn_vel.shape[1]])\n\n        \n        # two lstm models\n        flow_init, h_flow = self.lstm_flow_init(flow_sh)\n        vel_init, h_vel = self.lstm_vel_init(vel_sh)\n\n\n        # merge layer\n        # output\n        flow_vel_i = tf.concat([flow_init, vel_init], axis=2) # \n\n        flow_vel = self.dense_temporal_0(flow_vel_i)\n        flow_vel = self.dense_temporal_1(flow_vel)\n        flow_vel_f = self.dense_temporal_2(flow_vel)\n\n\n        # multi-head attention\n        # # Q: flow & vel\n        # # K: flow\n        att_flow, weight_flow = self.attention_flow(flow_vel_f, flow_init,\n                               return_attention_scores=True)\n\n\n        # # Q: vel & vel\n        # # K: vel\n        att_vel, weight_vel = self.attention_vel(flow_vel_f, vel_init,\n                               return_attention_scores=True)\n\n# -----\n\n        # concatenate covariates\n        # output flow_vel - covariates\n        fut_cov = tf.cast(fut_cov, dtype=tf.float32)\n        concat_flow = tf.concat([att_flow, fut_cov], axis=2)\n        concat_vel = tf.concat([att_vel, fut_cov], axis=2)\n  \n        # two models for flow and speed respectively\n\n        # flow\n        flow_final = self.lstm_flow_fin(concat_flow , initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) #  flow_vel_f, h_flow flow_vel_f[:,-1,:]\n        flow = self.drop_flow(flow_final)\n        flow = self.dense_flow_fin(flow)\n        flow = self.reshape(flow)\n\n\n        # velocity\n        vel_final = self.lstm_vel_fin(concat_vel, initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) \n        vel = self.drop_vel(vel_final)\n        vel = self.dense_vel_fin(vel)\n        vel = self.reshape(vel)\n\n\n        # concatenate two finals results\n        final = tf.concat([flow, vel], axis=-1)\n\n        return final, weight_flow, weight_vel\n\n\n# Create model\nmodel = GCN_Net()\noptimizer = Adam(lr=0.001)\nloss_fn = tf.keras.losses.MeanAbsoluteError()\n","metadata":{"executionInfo":{"elapsed":23753,"status":"ok","timestamp":1615724391770,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"oYHppaM9shP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adj_matrix = gcn_filter(adj_matrix, symmetric=True)","metadata":{"executionInfo":{"elapsed":33850,"status":"ok","timestamp":1615724401868,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"NAuLnUrpdmzz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del data_tr, data_te, data_flow, data_vel, data, scaled_tr, scaled_te\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function\n@tf.function\ndef train_on_batch(inputs, target, past_cov, fut_cov):\n    \n    loss = 0\n\n    with tf.GradientTape() as tape:\n        \n        predictions, weight_flow, weight_vel = model(inputs, adj_matrix, past_cov, fut_cov, training=True) #predictions\n        \n        loss = loss_fn(target, predictions)\n\n\n    variables = model.trainable_variables \n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n    \n    return loss","metadata":{"executionInfo":{"elapsed":33850,"status":"ok","timestamp":1615724401871,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"},"user_tz":-60},"id":"c-ua6TRjshP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 250\n\nstart = time.time()\n    \n# Keep results for plotting\ntrain_loss_results = []\n\nsamples_cov = list(loader_tr_cov)\n\nfor epoch in range(EPOCHS):\n    \n    epoch_loss_avg = tf.keras.metrics.Mean()\n     \n    step = 0\n\n    for batch in loader_tr:\n\n        cov = samples_cov[step]\n        past_cov = cov[0]\n        fut_cov = cov[1]\n        \n        # Training step\n        inputs, target = batch\n        \n        loss = train_on_batch(inputs, target, past_cov, fut_cov)\n        \n        # Track progress\n        epoch_loss_avg.update_state(loss)\n\n        step+=1\n\n    # End epoch\n    train_loss_results.append(epoch_loss_avg.result())\n\n    if epoch % 10 == 0:\n            print(\"Epoch {}: Loss MAE: {:.5f}\".format(epoch, epoch_loss_avg.result()))\n        \nprint('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"id":"mDbqw5X4shP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\nfig.suptitle('Training Metrics')\n\naxes.set_ylabel(\"Loss (MAE)\", fontsize=14)\naxes.plot(train_loss_results)\naxes.set_xlabel(\"Epoch\", fontsize=14)\nplt.show()","metadata":{"id":"igL4S8uAshP4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inverse_transform(forecasts, scaler):\n    # invert scaling\n    inv_pred = scaler.inverse_transform(forecasts)\n    return inv_pred","metadata":{"id":"5sPebFR-shP4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_forecasts(targets, forecasts, n_seq):\n    \n    list_rmse = []\n    list_mae = []\n    \n    for i in range(n_seq):\n        true = np.vstack([target[i] for target in targets])\n        predicted = np.vstack([forecast[i] for forecast in forecasts])\n        \n        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n        mae = np.absolute(true - predicted).mean(axis=0)\n        \n        list_rmse.append(rmse)\n        list_mae.append(mae)\n        \n    list_rmse = np.vstack(list_rmse)\n    list_mae = np.vstack(list_mae)\n    \n    return list_rmse, list_mae","metadata":{"id":"pHksYSD3GCkR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(attention):\n    \n    fig = plt.figure(figsize=(10,10))\n    plt.matshow(attention, cmap='viridis')\n\n    return plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom matplotlib import pyplot\n\nforecasts = []\ntargets = []\n\nrmse_list = []\nmae_list = []\n\nsamples_cov = list(loader_te_cov)\n\n\n# del loader_te_cov, loader_tr, loader_tr_cov\n# gc.collect()\n\n\nfor (step, (inp, targ)) in enumerate(tqdm(loader_te)):\n    \n            \n        timestamp = print_datetime[step]\n\n        cov = samples_cov[step]\n        past_cov = cov[0]\n        fut_cov = cov[1]\n        \n        pred, weight_flow, weight_vel  = model(inp, adj_matrix, past_cov, fut_cov, training=False)\n        \n\n#         columns_sums = np.vstack(weight_flow[0]).sum(axis=0)\n#         new_matrix = weight_flow[0]/columns_sums\n#         plot_attention(np.mean(new_matrix, axis=0))\n        \n#         fig = plt.figure(figsize=(10,10))\n#         pyplot.plot(range(12), np.mean(np.mean(inp[:, :, :-4524],axis=0),axis=1)) \n#         pyplot.bar(range(12), np.mean(np.mean(new_matrix, axis=0),axis=0).reshape(-1,), color ='red')\n#         pyplot.plot([None for i in np.mean(np.mean(inp[:, :, :-4524], axis=0), axis=1).tolist()] + [x for x in np.mean(pred[:, :, :-4524][0], axis=1).tolist()])\n#         pyplot.plot([None for i in np.mean(np.mean(inp[:, :, :-4524], axis=0), axis=1).tolist()] + [x for x in np.mean(targ[:, :, :-4524][0], axis=1).tolist()])\n#         plt.show()\n        \n\n        truth = inverse_transform(targ[0],  scaler)\n        pred = inverse_transform(pred[0],  scaler)       \n  \n        forecasts.append(pred)\n        targets.append(truth)\n            \n\n        rmse, mae = evaluate_forecasts(targets, forecasts, 12)\n        \n        rmse_list.append(rmse)\n        mae_list.append(mae)\n           \n        \n        del pred, truth, inp, targ, fut_cov, past_cov, cov\n        gc.collect()\n  ","metadata":{"id":"-VfOJ2z9shP4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(rmse_list)","metadata":{"id":"FcbFoft2A95Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(mae_list)","metadata":{"id":"sB4n8CvNBF-n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\nRMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n\nfor i in range(len(RMSE_MEAN)):\n    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n    print('')","metadata":{"id":"gEWDqYTZGNnP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\nMAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n\nfor i in range(len(MAE_MEAN)):\n    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n    print('')","metadata":{"id":"fMgrD1upGQ2o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('MTL_GCN_GRU_RMSE.pkl', 'wb') as f:  \n    pickle.dump(rmse_list, f)\n\nwith open('MTL_GCN_GRU_MAE.pkl', 'wb') as f:  \n    pickle.dump(mae_list, f)\n","metadata":{"id":"J86TlWyE1W9J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('FORECAST_ASTMTL.pkl', 'wb') as f:  \n    pickle.dump([forecasts,targets], f)\n","metadata":{"id":"iL4u98LoKOu8","trusted":true},"execution_count":null,"outputs":[]}]}