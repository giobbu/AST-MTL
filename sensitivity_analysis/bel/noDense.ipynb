{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"noDense.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YIBknzfcshPu"},"source":["## General Import"]},{"cell_type":"code","metadata":{"id":"YkNeBz0Cxbwt"},"source":["# !pip install geojson geopandas osmnx spektral matplotlib==3.1.3\n","\n","# from google.colab import files\n","# from google.colab import drive\n","\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7nfSj_GshPw"},"source":["from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt # plotting\n","import numpy as np # linear algebra\n","import os # accessing directory structure\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import gc\n","import time\n","import seaborn as sns; sns.set()\n","\n","import os \n","# Disable warnings, set Matplotlib inline plotting and load Pandas package\n","import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","#pd.options.display.mpl_style = 'default'\n","from datetime import datetime\n","import numpy as np\n","from datetime import datetime, timedelta\n","from pytz import timezone\n","from dateutil import tz\n","import geojson\n","import geopandas as gpd\n","from fiona.crs import from_epsg\n","import os, json\n","from shapely.geometry import shape, Point, Polygon, MultiPoint\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from geopandas.tools import sjoin\n","\n","import matplotlib.cm as cm\n","\n","import folium\n","import shapely.geometry\n","\n","from branca.colormap import  linear\n","import json\n","import branca.colormap as cm\n","import matplotlib.colors as colors\n","%matplotlib inline\n","\n","import networkx as nx\n","import pickle\n","\n","import osmnx as ox\n","ox.config(log_console=True, use_cache=True)\n","ox.__version__\n","\n","import matplotlib.colors as mcolors\n","import gc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b275b4IDshPx"},"source":["import tensorflow as tf\n","from tensorflow.keras import Model\n","from spektral.layers import GCNConv, ChebConv #(channels, K=1)\n","from spektral.utils import gcn_filter\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhWfC-IG3Tkq"},"source":["from numpy.random import seed\n","\n","# Reproducability\n","def set_seed(seed=31415):\n","    \n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","    \n","set_seed(31415)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmokpg02shPy"},"source":["path_graph = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/direct graph/GRAPH_ADJ.pkl'\n","\n","path_feat_flow = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/direct graph/FEATURES_FLOW.csv'\n","path_feat_vel = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/direct graph/FEATURES_VEL.csv'\n","\n","with open(path_graph,'rb') as f:\n","    graph, adj_matrix, edges, G = pickle.load(f)\n","\n","adj_mx = nx.to_numpy_matrix(graph)\n","\n","# flow\n","features_flow = pd.read_csv(path_feat_flow).iloc[:,1:].values\n","# vel\n","features_vel = pd.read_csv(path_feat_vel).iloc[:,1:].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4R-q2lbshPy"},"source":["# adj_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"libFCZUSshPy"},"source":["# features_flow.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Xw3TcIrmEJA"},"source":["# features_vel.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHtO7pnnxPQG"},"source":["# PARAMETERS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sduHFHSexOTP"},"source":["inputs = 12\n","\n","batch_train = 32 # best 32\n","batch_test = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UvuJcrvshP0"},"source":["## split train/val/test"]},{"cell_type":"code","metadata":{"id":"nd6kl3lOmOth"},"source":["data_flow = features_flow[:, :-1]\n","data_vel = features_vel[:, :-1]\n","\n","data = np.concatenate([data_flow, data_vel], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAuBHy0mdO5"},"source":["# data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqWVkLFlshP0"},"source":["# Train/test split\n","data_tr, data_te = data[:-168*2*2, :], data[-168*2*2:, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcOJcaqSvih_"},"source":["# Covariates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5GB9RXbYsoga"},"source":["datetime = features_flow[:, -1]\n","\n","# for figures\n","print_datetime = datetime[-168*2*2+inputs:]\n","\n","DATETIME = pd.DataFrame(datetime, columns=['Datetime'])\n","DATETIME['Datetime'] = pd.to_datetime(DATETIME['Datetime'])\n","\n","DATETIME['minutes'] = DATETIME['Datetime'].dt.minute\n","DATETIME['hour'] = DATETIME['Datetime'].dt.hour\n","\n","DATETIME['hour_x']=np.sin(DATETIME.hour*(2.*np.pi/23))\n","DATETIME['hour_y']=np.cos(DATETIME.hour*(2.*np.pi/23))\n","\n","DATETIME['day'] = DATETIME['Datetime'].dt.day\n","DATETIME['DayOfWeek'] = DATETIME['Datetime'].dt.dayofweek\n","\n","DATETIME['WorkingDays'] = DATETIME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\n","DATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\n","DATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n","\n","DATETIME = DATETIME.drop(['Datetime','minutes','hour','day'], axis=1).values\n","\n","# temporal features = 4\n","feat_time = 4\n","\n","# datetime Train/test split\n","time_tr, time_te = DATETIME[:-168*2*2, :], DATETIME[-168*2*2:, :]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1BC-dhtWvkv","executionInfo":{"status":"ok","timestamp":1614957522232,"user_tz":-60,"elapsed":12528,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"}},"outputId":"50708af1-4b55-40da-cfac-f6ca13ddbd30"},"source":["del data, data_flow, data_vel, features_flow, features_vel\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["64"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"rnkgB_FYshP0"},"source":["## scale data"]},{"cell_type":"code","metadata":{"id":"RjIvQ0QgshP1"},"source":["scaler = MinMaxScaler(feature_range=(0, 1))\n","scaler_cov = MinMaxScaler(feature_range=(0, 1))\n","\n","# fit and transform\n","scaled_tr = scaler.fit_transform(data_tr)\n","# transform\n","scaled_te = scaler.transform(data_te)\n","\n","# fit and transform\n","scaled_tr_cov = scaler_cov.fit_transform(time_tr)\n","# transform\n","scaled_te_cov = scaler_cov.transform(time_te)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nsmXb48sshP1"},"source":["## prepare data for deep learning"]},{"cell_type":"code","metadata":{"id":"sQiniFq0shP2"},"source":["def prepare_data_DL(INPUT, FEAT, BATCH):\n","    \n","    dataset = FEAT.reshape(FEAT.shape[0], FEAT.shape[1]) \n","    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n","\n","    inputs = dataset.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n","    inputs = inputs.flat_map(lambda window: window.batch(INPUT))\n","\n","    targets = dataset.window(INPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n","    targets = targets.flat_map(lambda window: window.batch(INPUT))\n","\n","    dataset = tf.data.Dataset.zip((inputs, targets))\n","    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIs0wUItshP2"},"source":["# features\n","loader_tr = prepare_data_DL(inputs, scaled_tr, batch_train)\n","loader_te = prepare_data_DL(inputs, scaled_te, batch_test)\n","\n","# covariates\n","loader_tr_cov = prepare_data_DL(inputs, scaled_tr_cov, batch_train)\n","loader_te_cov = prepare_data_DL(inputs, scaled_te_cov, batch_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssey4RtN0o8m"},"source":["        # # encoder\n","        # self.e_tcn = tf.keras.layers.Conv1D(150, 2, padding ='same', activation ='relu')\n","        # self.pool = tf.keras.layers.MaxPool1D(pool_size=2, padding='same')\n","\n","        # self.e_tcn_0 = tf.keras.layers.Conv1D(32, 2, padding ='same', activation ='relu')\n","        # self.pool_0 = tf.keras.layers.MaxPool1D(pool_size=2, padding='same')\n"," \n","        # # decoder\n","        # self.upsample_0 = tf.keras.layers.UpSampling1D(size=2)\n","        # self.d_tcn_0 = tf.keras.layers.Conv1D(32, 2, padding ='same', activation ='relu')\n","\n","        # self.upsample = tf.keras.layers.UpSampling1D(size=2)\n","        # self.d_tcn = tf.keras.layers.Conv1D(150, 2, padding ='same', activation ='relu')\n","\n","\n","                # encoder\n","        # flow_vel = self.e_tcn(flow_vel)\n","        # flow_vel = self.pool(flow_vel)\n","\n","        # flow_vel = self.e_tcn_0(flow_vel)\n","        # flow_vel = self.pool_0(flow_vel)\n","        \n","        # decoder\n","        # flow_vel = self.upsample_0(flow_vel)\n","        # flow_vel = self.d_tcn_0(flow_vel)\n","\n","        # flow_vel = self.upsample(flow_vel)\n","        # flow_vel = self.d_tcn(flow_vel)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edFVyS_f4ehR"},"source":["        # self.conv = tf.keras.layers.Conv1D(150, 2, padding ='same')\n","\n","        # # block 1\n","        # self.conv_tanh = tf.keras.layers.Conv1D(150, 2, padding ='causal', activation = 'tanh')\n","        # self.conv_sigm = tf.keras.layers.Conv1D(150, 2, padding ='causal', activation = 'sigmoid') \n","\n","        # # temporal convolution\n","        # flow_vel = self.conv(flow_vel)\n","\n","        # # block 1\n","        # tanh = self.conv_tanh(flow_vel)\n","        # sigm = self.conv_sigm(flow_vel)\n","        # gate = self.mult([tanh, sigm])\n","        # flow_vel = self.add([flow_vel, gate])\n","\n","        # self.mult = tf.keras.layers.Multiply()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88UwXzIwuVGs"},"source":["        # flow_vel = self.dense_temporal_0(flow_vel)\n","        # flow_vel = self.dense_temporal_1(flow_vel)\n","        # flow_vel = self.dense_temporal_2(flow_vel)\n","\n","# self.dense_temporal_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(150))\n","#         self.dense_temporal_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(50))\n","#         self.dense_temporal_2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(150))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYHppaM9shP3","colab":{"base_uri":"https://localhost:8080/","height":130},"executionInfo":{"status":"error","timestamp":1614957524667,"user_tz":-60,"elapsed":14906,"user":{"displayName":"giovanni buroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjarG9foST_wpJ_PfvHTR0iWGuChdFnmm4ong7n=s64","userId":"06127736531396060751"}},"outputId":"3d36e698-2754-4bb4-83c3-a09e99800677"},"source":["l2_reg = 5e-4  # Regularization rate for l2\n","\n","# Build model\n","class GCN_Net(Model):\n","    \n","    def __init__(self, **kwargs):\n","        \n","        super().__init__(**kwargs)\n","\n","\n","\n","        self.dense_start = tf.keras.layers.Dense(5795)\n","\n","\n","        # GCN 1st order approximation\n","        self.gcn_flow_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","        self.gcn_flow_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","\n","        self.gcn_vel_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","        self.gcn_vel_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","\n","        # # cheb polinomial\n","        # self.cheb_flow = ChebConv(12,  2, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","        # self.cheb_vel = ChebConv(12, 2, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n","        \n","\n","\n","###################\n","\n","        # encoder GRU FLOW\n","        self.lstm_cells_flow_init = tf.keras.layers.GRUCell(150, \n","                                        kernel_initializer='glorot_uniform',\n","                                        recurrent_initializer='glorot_uniform',\n","                                        kernel_regularizer=regularizers.l2(0.001),\n","                                        bias_initializer='zeros', dropout=0.0) \n","        \n","        self.lstm_flow_init = tf.keras.layers.RNN(self.lstm_cells_flow_init, return_sequences = True, return_state =True)\n","\n","        # self.lstm_flow_init = tf.compat.v1.keras.layers.CuDNNGRU(150,\n","        #                                 kernel_initializer='glorot_uniform',\n","        #                                 recurrent_initializer='glorot_uniform',\n","        #                                 kernel_regularizer=regularizers.l2(0.001),\n","        #                                 bias_initializer='zeros', return_sequences = True, return_state =True)\n","\n","        # decoder GRU FLOW\n","        self.lstm_cells_flow_fin = tf.keras.layers.GRUCell(150, \n","                                        kernel_initializer='glorot_uniform',\n","                                        recurrent_initializer='glorot_uniform',\n","                                        kernel_regularizer=regularizers.l2(0.001),\n","                                        bias_initializer='zeros', dropout=0.0) \n","        \n","        self.lstm_flow_fin = tf.keras.layers.RNN(self.lstm_cells_flow_fin)\n","\n","        # self.lstm_flow_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, \n","        #                                 kernel_initializer='glorot_uniform',\n","        #                                 recurrent_initializer='glorot_uniform',\n","        #                                 kernel_regularizer=regularizers.l2(0.001),\n","        #                                 bias_initializer='zeros')\n","\n","\n","\n","        self.drop_flow = tf.keras.layers.Dropout(0.1)\n","\n","        self.dense_flow_fin = tf.keras.layers.Dense(5975*12,\n","                                           kernel_regularizer=regularizers.l2(0.001))\n","        \n","        \n","        # self.dense_flow_fin = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5975,\n","        #                                    kernel_regularizer=regularizers.l2(0.001)))\n","        \n","\n","\n","\n","\n","##########################\n","\n","        # encoder GRU VEL\n","        self.lstm_cells_vel_init = tf.keras.layers.GRUCell(150, #activation ='relu',\n","                                        kernel_initializer='glorot_uniform',\n","                                        recurrent_initializer='glorot_uniform',\n","                                        kernel_regularizer=regularizers.l2(0.001),\n","                                        bias_initializer='zeros', dropout=0.0) \n","        \n","        self.lstm_vel_init = tf.keras.layers.RNN(self.lstm_cells_vel_init, return_sequences = True, return_state =True)\n","\n","        # self.lstm_vel_init = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n","        #                                 kernel_initializer='glorot_uniform',\n","        #                                 recurrent_initializer='glorot_uniform',\n","        #                                 kernel_regularizer=regularizers.l2(0.001),\n","        #                                 bias_initializer='zeros', return_sequences = True, return_state =True)\n","\n","\n","        # decoder GRU VEL\n","        self.lstm_cells_vel_fin = tf.keras.layers.GRUCell(150, #activation ='relu',\n","                                        kernel_initializer='glorot_uniform',\n","                                        recurrent_initializer='glorot_uniform',\n","                                        kernel_regularizer=regularizers.l2(0.001),\n","                                        bias_initializer='zeros', dropout=0.0) \n","        \n","        self.lstm_vel_fin = tf.keras.layers.RNN(self.lstm_cells_vel_fin)\n","\n","\n","        # self.lstm_vel_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n","        #                                 kernel_initializer='glorot_uniform',\n","        #                                 recurrent_initializer='glorot_uniform',\n","        #                                 kernel_regularizer=regularizers.l2(0.001),\n","        #                                 bias_initializer='zeros')\n","\n","\n","\n","        self.drop_vel = tf.keras.layers.Dropout(0.1)\n","\n","\n","        self.dense_vel_fin = tf.keras.layers.Dense(5975*12,\n","                                           kernel_regularizer=regularizers.l2(0.001))\n","        \n","        \n","        # self.dense_vel_fin = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5975,\n","        #                                    kernel_regularizer=regularizers.l2(0.001)))\n","        \n","        \n","##################################\n","\n","        self.attention_flow = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4)\n","        self.attention_vel = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4)\n","\n","\n","        self.dense_temporal_0 = tf.keras.layers.Dense(150, activation ='relu') #tf.keras.layers.TimeDistributed()\n","        self.dense_temporal_1 = tf.keras.layers.Dense(64, activation ='relu')\n","        self.dense_temporal_2 = tf.keras.layers.Dense(150)\n","\n","        self.drop = tf.keras.layers.Dropout(0.1)   \n","\n","        self.add = tf.keras.layers.Add()\n","        self.norm = tf.keras.layers.LayerNormalization()\n","\n","        self.reshape = tf.keras.layers.Reshape([12, 5975])\n","        self.repeat = tf.keras.layers.RepeatVector(12)\n","        \n","\n","    def call(self, flow_vel, a, past_cov, fut_cov):\n","\n","        # sparse matrix\n","        sparse_a = tf.sparse.from_dense(a)\n","        \n","        past_cov = tf.cast(past_cov, dtype=tf.float32)\n","\n","        # lapl_a = gcn_filter(sparse_a, symmetric=True)\n","        # Cheb_a = chebyshev_filter(sparse_a, 2, symmetric=True)\n","\n","\n","\n","        # flow\n","        flow_0 = flow_vel[:, :, :-5975]\n","        # velocity\n","        vel_0  = flow_vel[:, :, -5975:]\n","        \n","\n","        # shape for gcn\n","        flow_sh = tf.reshape(flow_0 , [flow_0.shape[0], flow_0 .shape[2], flow_0.shape[1]])\n","        vel_sh = tf.reshape(vel_0 , [vel_0.shape[0], vel_0.shape[2], vel_0.shape[1]])\n","\n","        # two gcn on flow and vel\n","        gcn_flow = self.gcn_flow_0_enc([flow_sh, sparse_a])\n","        gcn_flow = self.gcn_flow_1_enc([gcn_flow, sparse_a])\n","\n","        # gcn_flow = self.cheb_flow([flow_sh, sparse_a])\n","\n","        gcn_vel = self.gcn_vel_0_enc([vel_sh, sparse_a])\n","        gcn_vel = self.gcn_vel_1_enc([gcn_vel, sparse_a])\n","\n","        # gcn_vel = self.cheb_vel([vel_sh, sparse_a])\n","\n","        # shape for lstm\n","        flow_sh = tf.reshape(gcn_flow, [gcn_flow.shape[0], gcn_flow.shape[2], gcn_flow.shape[1]])\n","        vel_sh = tf.reshape(gcn_vel, [gcn_vel.shape[0], gcn_vel.shape[2], gcn_vel.shape[1]])\n","\n","\n","        \n","        # two lstm models\n","        flow_init, h_flow = self.lstm_flow_init(flow_sh)\n","        vel_init, h_vel = self.lstm_vel_init(vel_sh)\n","\n","\n","\n","        # multi-head attention\n","        # # Q: flow\n","        # # K: flow & vel\n","        # att_flow = self.attention_flow(flow_init, add_flow_vel) #\n","        att_flow = self.attention_flow(vel_init, flow_init)\n","\n","\n","        # # Q: vel\n","        # # K: flow & vel\n","        # att_vel = self.attention_vel(vel_init, add_flow_vel)\n","        att_vel = self.attention_vel(flow_init, vel_init)\n","\n","        # add\n","        add_flow = self.add([att_flow, vel_init]) # flow_vel, flow_init\n","        add_vel = self.add([att_vel, flow_init]) # vel_init\n","\n","\n","\n","        # concatenate covariates\n","        # output flow_vel - covariates\n","        fut_cov = tf.cast(fut_cov, dtype=tf.float32)\n","        concat_flow = tf.concat([add_flow, fut_cov], axis=2)\n","        concat_vel = tf.concat([add_vel, fut_cov], axis=2)\n","  \n","        # two models for flow and speed respectively\n","\n","        # flow\n","        flow_final = self.lstm_flow_fin(concat_flow , initial_state = [h_flow]) # , initial_state = [flow_vel_f]) #  flow_vel_f, h_flow flow_vel_f[:,-1,:]\n","        flow = self.drop_flow(flow_final)\n","        flow = self.dense_flow_fin(flow)\n","        flow = self.reshape(flow)\n","\n","\n","        # velocity\n","        vel_final = self.lstm_vel_fin(concat_vel, initial_state = [h_vel]) # , initial_state = [flow_vel_f]) \n","        vel = self.drop_vel(vel_final)\n","        vel = self.dense_vel_fin(vel)\n","        vel = self.reshape(vel)\n","\n","\n","        # concatenate two finals results\n","        final = tf.concat([flow, vel], axis=-1)\n","\n","        return final\n","\n","\n","# Create model\n","model = GCN_Net()\n","optimizer = Adam(lr=0.001)\n","loss_fn = tf.keras.losses.MeanAbsoluteError()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-110bfe7b9a33>\"\u001b[0;36m, line \u001b[0;32m214\u001b[0m\n\u001b[0;31m    flow = self.drop_flow(flow_final)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"NAuLnUrpdmzz"},"source":["adj_matrix = gcn_filter(adj_matrix, symmetric=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-ua6TRjshP3"},"source":["# Training function\n","@tf.function\n","def train_on_batch(inputs, target, past_cov, fut_cov):\n","    \n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        \n","        predictions = model(inputs, adj_matrix, past_cov, fut_cov, training=True)\n","        \n","        loss = loss_fn(target, predictions)\n","\n","    variables = model.trainable_variables \n","\n","    gradients = tape.gradient(loss, variables)\n","\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mDbqw5X4shP3"},"source":["EPOCHS = 250\n","\n","# Keep results for plotting\n","train_loss_results = []\n","\n","samples_cov = list(loader_tr_cov)\n","\n","for epoch in range(EPOCHS):\n","    \n","    start = time.time()\n","\n","    epoch_loss_avg = tf.keras.metrics.Mean()\n","     \n","    step = 0\n","\n","    for batch in loader_tr:\n","\n","        cov = samples_cov[step]\n","        past_cov = cov[0]\n","        fut_cov = cov[1]\n","        \n","        # Training step\n","        inputs, target = batch\n","        \n","        loss = train_on_batch(inputs, target, past_cov, fut_cov)\n","        \n","        # Track progress\n","        epoch_loss_avg.update_state(loss)\n","\n","        step+=1\n","\n","    # End epoch\n","    train_loss_results.append(epoch_loss_avg.result())\n","\n","    if epoch % 10 == 0:\n","            print(\"Epoch {}: Loss MAE: {:.4f}\".format(epoch, epoch_loss_avg.result()))\n","        \n","print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igL4S8uAshP4"},"source":["fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\n","fig.suptitle('Training Metrics')\n","\n","axes.set_ylabel(\"Loss (MAE)\", fontsize=14)\n","axes.plot(train_loss_results)\n","axes.set_xlabel(\"Epoch\", fontsize=14)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sPebFR-shP4"},"source":["def inverse_transform(forecasts, scaler):\n","    # invert scaling\n","    inv_pred = scaler.inverse_transform(forecasts)\n","    return inv_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHksYSD3GCkR"},"source":["def evaluate_forecasts(targets, forecasts, n_seq):\n","    \n","    list_rmse = []\n","    list_mae = []\n","    \n","    for i in range(n_seq):\n","        true = np.vstack([target[i] for target in targets])\n","        predicted = np.vstack([forecast[i] for forecast in forecasts])\n","        \n","        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n","        mae = np.absolute(true - predicted).mean(axis=0)\n","        \n","        list_rmse.append(rmse)\n","        list_mae.append(mae)\n","        \n","    list_rmse = np.vstack(list_rmse)\n","    list_mae = np.vstack(list_mae)\n","    \n","    return list_rmse, list_mae"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VfOJ2z9shP4"},"source":["forecasts = []\n","targets = []\n","\n","rmse_list = []\n","mae_list = []\n","\n","samples_cov = list(loader_te_cov)\n","\n","    \n","for (step, (inp, targ)) in enumerate(loader_te):\n","    \n","            \n","        timestamp = print_datetime[step]\n","        print(timestamp)\n","\n","        cov = samples_cov[step]\n","        past_cov = cov[0]\n","        fut_cov = cov[1]\n","        \n","        pred  = model(inp, adj_matrix, past_cov, fut_cov, training=False)\n","        print(pred.shape)\n","        \n","        truth = inverse_transform(targ[0],  scaler)\n","        pred = inverse_transform(pred[0],  scaler)\n","        \n","        forecasts.append(pred)\n","        targets.append(truth)\n","\n","        rmse, mae = evaluate_forecasts(targets, forecasts, 12)\n","           \n","        rmse_list.append(rmse)\n","        mae_list.append(mae)\n","\n","         \n","        # fig = plt.figure(figsize=(10, 5))   \n","        # plt.plot(np.sum(pred[:, :5975], axis=1), label='Prediction') \n","        # plt.plot(np.sum(truth[:, :5975], axis=1), label='Truth') \n","        # plt.title('Sum flow Prediction on all highways in Belgium')\n","        # plt.legend()\n","        # plt.show()\n","        # fig.clear()\n","        # plt.close(fig)       \n","        # plt.show()\n","\n","        # fig = plt.figure(figsize=(10, 5))   \n","        # plt.plot(np.mean(pred[:, 5975:], axis=1), label='Prediction') \n","        # plt.plot(np.mean(truth[:, 5975:], axis=1), label='Truth') \n","        # plt.title('Sum Velocity Prediction on all highways in Belgium')\n","        # plt.legend()\n","        # plt.show()\n","        # fig.clear()\n","        # plt.close(fig)       \n","        # plt.show()\n","\n","        print('* Time step '+str(step))\n","        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n","        print('----')\n","        \n","        \n","        new_instance = scaled_te[step,:].reshape(1,-1)\n","        new_instance_cov = scaled_te_cov[step,:].reshape(1,-1)\n","    \n","        scaled_tr = np.vstack([scaled_tr[1:], new_instance])\n","        scaled_tr_cov = np.vstack([scaled_tr_cov[1:], new_instance_cov])\n","        \n","        loader_new = prepare_data_DL(12, scaled_tr, batch_train)\n","        loader_new_cov = prepare_data_DL(12, scaled_tr_cov, batch_train)\n","        \n","        UPDATE = 2\n","        time_update = 168*2\n","\n","        if step % time_update == 0:\n","\n","          print('')\n","          print('* Time to UPDATE the Model after '+str(time_update)+' steps')\n","          print('')\n","          sample_cov_new = list(loader_new_cov) \n","          \n","\n","          for epoch in range(UPDATE):\n","\n","            step_new = 0\n","            \n","            for batch in loader_new:\n","\n","              cov_new = sample_cov_new[step_new]\n","\n","              past_cov_new = cov_new[0]\n","              future_cov_new = cov_new[1]\n","              \n","              inp_new, targ_new = batch\n","\n","              loss = train_on_batch(inp_new, targ_new, past_cov_new, future_cov_new)\n","\n","              # Track progress\n","              epoch_loss_avg.update_state(loss)\n","\n","              # End epoch\n","              train_loss_results.append(epoch_loss_avg.result())\n","\n","              step_new+=1\n","\n","            \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcbFoft2A95Q"},"source":["np.mean(rmse_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sB4n8CvNBF-n"},"source":["np.mean(mae_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEWDqYTZGNnP"},"source":["RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\n","RMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n","\n","for i in range(len(RMSE_MEAN)):\n","    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n","    print('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMgrD1upGQ2o"},"source":["MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\n","MAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n","\n","for i in range(len(MAE_MEAN)):\n","    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n","    print('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J86TlWyE1W9J"},"source":["with open('noDense_RMSE.pkl', 'wb') as f:  \n","    pickle.dump(rmse_list, f)\n","\n","!cp noDense_RMSE.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n","print('RMSE')\n","\n","\n","with open('noDense_MAE.pkl', 'wb') as f:  \n","    pickle.dump(mae_list, f)\n","\n","!cp noDense_MAE.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n","print('MAE')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iL4u98LoKOu8"},"source":["with open('MTL_training.pkl', 'wb') as f:  \n","    pickle.dump(train_loss_results, f)\n","\n","!cp MTL_training.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n","print('Training Process')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ocz_E41x9wHC"},"source":["while True:pass"],"execution_count":null,"outputs":[]}]}
