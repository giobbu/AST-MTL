{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ast-mtl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EU9ha7pM1iQ"
      },
      "source": [
        "# drive mount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkNeBz0Cxbwt"
      },
      "source": [
        "# !pip install geojson geopandas osmnx spektral matplotlib==3.1.3\n",
        "\n",
        "# from google.colab import files\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqN83EQxMu0G"
      },
      "source": [
        "# general import"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nfSj_GshPw"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import os # accessing directory structure\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import gc\n",
        "import time\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "import os \n",
        "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#pd.options.display.mpl_style = 'default'\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "from dateutil import tz\n",
        "import geojson\n",
        "import geopandas as gpd\n",
        "from fiona.crs import from_epsg\n",
        "import os, json\n",
        "from shapely.geometry import shape, Point, Polygon, MultiPoint\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from geopandas.tools import sjoin\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import folium\n",
        "import shapely.geometry\n",
        "\n",
        "from branca.colormap import  linear\n",
        "import json\n",
        "import branca.colormap as cm\n",
        "import matplotlib.colors as colors\n",
        "%matplotlib inline\n",
        "\n",
        "import networkx as nx\n",
        "import pickle\n",
        "\n",
        "import osmnx as ox\n",
        "ox.config(log_console=True, use_cache=True)\n",
        "ox.__version__\n",
        "\n",
        "import matplotlib.colors as mcolors\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b275b4IDshPx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from spektral.layers import GCNConv, ChebConv #(channels, K=1)\n",
        "from spektral.utils import gcn_filter\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-HehR9JM6Dt"
      },
      "source": [
        "# for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhWfC-IG3Tkq"
      },
      "source": [
        "from numpy.random import seed\n",
        "\n",
        "# Reproducability\n",
        "def set_seed(seed=31415):\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    \n",
        "set_seed(31415)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KshcMDinM__8"
      },
      "source": [
        "# imporing files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmokpg02shPy"
      },
      "source": [
        "path_graph = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/bxl graph/graph_adj.pkl'\n",
        "\n",
        "path_feat_flow = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/bxl graph/FEATURES_FLOW.csv'\n",
        "path_feat_vel = 'drive/MyDrive/Colab Notebooks/MTL traffic forecasting/bxl graph/FEATURES_VEL.csv'\n",
        "\n",
        "with open(path_graph,'rb') as f:\n",
        "    graph, adj_matrix, edges, G = pickle.load(f)\n",
        "\n",
        "adj_mx = nx.to_numpy_matrix(graph)\n",
        "\n",
        "# flow\n",
        "features_flow = pd.read_csv(path_feat_flow).iloc[:,1:].values\n",
        "# vel\n",
        "features_vel = pd.read_csv(path_feat_vel).iloc[:,1:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHtO7pnnxPQG"
      },
      "source": [
        "# PARAMETERS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sduHFHSexOTP"
      },
      "source": [
        "inputs = 12\n",
        "granularity = 2*2\n",
        "\n",
        "batch_train = 32 # best 32\n",
        "batch_test = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UvuJcrvshP0"
      },
      "source": [
        "## split train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd6kl3lOmOth"
      },
      "source": [
        "data_flow = features_flow[:, :-1]\n",
        "data_vel = features_vel[:, :-1]\n",
        "\n",
        "data = np.concatenate([data_flow, data_vel], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqWVkLFlshP0"
      },
      "source": [
        "# Train/test split\n",
        "data_tr, data_te = data[:-168*2*granularity, :], data[-168*2*granularity:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcOJcaqSvih_"
      },
      "source": [
        "# time-based Covariates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GB9RXbYsoga"
      },
      "source": [
        "datetime = features_flow[:, -1]\n",
        "\n",
        "# for figures\n",
        "print_datetime = datetime[-168*2*granularity+inputs:]\n",
        "\n",
        "DATETIME = pd.DataFrame(datetime, columns=['Datetime'])\n",
        "DATETIME['Datetime'] = pd.to_datetime(DATETIME['Datetime'])\n",
        "\n",
        "DATETIME['minutes'] = DATETIME['Datetime'].dt.minute\n",
        "DATETIME['hour'] = DATETIME['Datetime'].dt.hour\n",
        "\n",
        "DATETIME['hour_x']=np.sin(DATETIME.hour*(2.*np.pi/23))\n",
        "DATETIME['hour_y']=np.cos(DATETIME.hour*(2.*np.pi/23))\n",
        "\n",
        "DATETIME['day'] = DATETIME['Datetime'].dt.day\n",
        "DATETIME['DayOfWeek'] = DATETIME['Datetime'].dt.dayofweek\n",
        "\n",
        "DATETIME['WorkingDays'] = DATETIME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\n",
        "DATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\n",
        "DATETIME['WorkingDays'] = DATETIME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n",
        "\n",
        "DATETIME = DATETIME.drop(['Datetime','minutes','hour','day'], axis=1).values\n",
        "\n",
        "# temporal features = 4\n",
        "feat_time = 4\n",
        "\n",
        "# datetime Train/test split\n",
        "time_tr, time_te = DATETIME[:-168*2*granularity, :], DATETIME[-168*2*granularity:, :]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnkgB_FYshP0"
      },
      "source": [
        "## scale data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjIvQ0QgshP1"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_cov = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# fit and transform\n",
        "scaled_tr = scaler.fit_transform(data_tr)\n",
        "# transform\n",
        "scaled_te = scaler.transform(data_te)\n",
        "\n",
        "# fit and transform\n",
        "scaled_tr_cov = scaler_cov.fit_transform(time_tr)\n",
        "# transform\n",
        "scaled_te_cov = scaler_cov.transform(time_te)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsmXb48sshP1"
      },
      "source": [
        "## prepare data for deep learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQiniFq0shP2"
      },
      "source": [
        "def prepare_data_DL(INPUT, FEAT, BATCH):\n",
        "    \n",
        "    dataset = FEAT.reshape(FEAT.shape[0], FEAT.shape[1]) \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
        "\n",
        "    inputs = dataset.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n",
        "    inputs = inputs.flat_map(lambda window: window.batch(INPUT))\n",
        "\n",
        "    targets = dataset.window(INPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n",
        "    targets = targets.flat_map(lambda window: window.batch(INPUT))\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
        "    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIs0wUItshP2"
      },
      "source": [
        "# features\n",
        "loader_tr = prepare_data_DL(inputs, scaled_tr, batch_train)\n",
        "loader_te = prepare_data_DL(inputs, scaled_te, batch_test)\n",
        "\n",
        "# covariates\n",
        "loader_tr_cov = prepare_data_DL(inputs, scaled_tr_cov, batch_train)\n",
        "loader_te_cov = prepare_data_DL(inputs, scaled_te_cov, batch_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYHppaM9shP3"
      },
      "source": [
        "l2_reg = 5e-4  # Regularization rate for l2\n",
        "\n",
        "# Build model\n",
        "class GCN_Net(Model):\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.street = 4524\n",
        "\n",
        "\n",
        "        # GCN 1st order approximation\n",
        "        self.gcn_flow_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n",
        "        self.gcn_flow_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n",
        "\n",
        "        self.gcn_vel_0_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n",
        "        self.gcn_vel_1_enc = GCNConv(12, activation=\"relu\", kernel_regularizer=l2(l2_reg), use_bias=False)\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "        # encoder GRU FLOW\n",
        "        # self.lstm_cells_flow_init = tf.keras.layers.GRUCell(150, activation ='relu',\n",
        "        #                                 kernel_initializer='glorot_uniform',\n",
        "        #                                 recurrent_initializer='glorot_uniform',\n",
        "        #                                 kernel_regularizer=regularizers.l2(0.001),\n",
        "        #                                 bias_initializer='zeros', dropout=0.0) \n",
        "        \n",
        "        # self.lstm_flow_init = tf.keras.layers.RNN(self.lstm_cells_flow_init, return_sequences = True, return_state =True)\n",
        "\n",
        "        self.lstm_flow_init = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n",
        "                                        kernel_initializer='glorot_uniform',\n",
        "                                        recurrent_initializer='glorot_uniform',\n",
        "                                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n",
        "\n",
        "        # decoder GRU FLOW\n",
        "        # self.lstm_cells_flow_fin = tf.keras.layers.GRUCell(150, activation ='relu',\n",
        "        #                                 kernel_initializer='glorot_uniform',\n",
        "        #                                 recurrent_initializer='glorot_uniform',\n",
        "        #                                 kernel_regularizer=regularizers.l2(0.001),\n",
        "        #                                 bias_initializer='zeros', dropout=0.0) \n",
        "        \n",
        "        # self.lstm_flow_fin = tf.keras.layers.RNN(self.lstm_cells_flow_fin)\n",
        "\n",
        "        self.lstm_flow_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n",
        "                                        kernel_initializer='glorot_uniform',\n",
        "                                        recurrent_initializer='glorot_uniform',\n",
        "                                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                                        bias_initializer='zeros')\n",
        "\n",
        "\n",
        "\n",
        "        self.drop_flow = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "        self.dense_flow_fin = tf.keras.layers.Dense(self.street*12,\n",
        "                                           kernel_regularizer=regularizers.l2(0.001))\n",
        "        \n",
        "        \n",
        "\n",
        "##########################\n",
        "\n",
        "        # encoder GRU VEL\n",
        "        # self.lstm_cells_vel_init = tf.keras.layers.GRUCell(150, activation ='relu',\n",
        "        #                                 kernel_initializer='glorot_uniform',\n",
        "        #                                 recurrent_initializer='glorot_uniform',\n",
        "        #                                 kernel_regularizer=regularizers.l2(0.001),\n",
        "        #                                 bias_initializer='zeros', dropout=0.0) \n",
        "        \n",
        "        # self.lstm_vel_init = tf.keras.layers.RNN(self.lstm_cells_vel_init, return_sequences = True, return_state =True)\n",
        "\n",
        "        self.lstm_vel_init = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n",
        "                                        kernel_initializer='glorot_uniform',\n",
        "                                        recurrent_initializer='glorot_uniform',\n",
        "                                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                                        bias_initializer='zeros', return_sequences = True, return_state =True)\n",
        "\n",
        "\n",
        "        # decoder GRU VEL\n",
        "        # self.lstm_cells_vel_fin = tf.keras.layers.GRUCell(150, activation ='relu',\n",
        "        #                                 kernel_initializer='glorot_uniform',\n",
        "        #                                 recurrent_initializer='glorot_uniform',\n",
        "        #                                 kernel_regularizer=regularizers.l2(0.001),\n",
        "        #                                 bias_initializer='zeros', dropout=0.0) \n",
        "        \n",
        "        # self.lstm_vel_fin = tf.keras.layers.RNN(self.lstm_cells_vel_fin)\n",
        "\n",
        "\n",
        "        self.lstm_vel_fin = tf.compat.v1.keras.layers.CuDNNGRU(150, #activation ='relu',\n",
        "                                        kernel_initializer='glorot_uniform',\n",
        "                                        recurrent_initializer='glorot_uniform',\n",
        "                                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                                        bias_initializer='zeros')\n",
        "\n",
        "\n",
        "\n",
        "        self.drop_vel = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "\n",
        "        self.dense_vel_fin = tf.keras.layers.Dense(self.street*12,\n",
        "                                           kernel_regularizer=regularizers.l2(0.001))\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "##################################\n",
        "\n",
        "        self.attention_flow = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4)\n",
        "        self.attention_vel = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4)\n",
        "\n",
        "\n",
        "        self.dense_temporal_0 = tf.keras.layers.Dense(32, activation ='relu') #tf.keras.layers.TimeDistributed()\n",
        "        self.dense_temporal_1 = tf.keras.layers.Dense(32, activation ='relu')\n",
        "        self.dense_temporal_2 = tf.keras.layers.Dense(150)\n",
        "\n",
        "        self.drop = tf.keras.layers.Dropout(0.1)   \n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "        self.reshape = tf.keras.layers.Reshape([12, self.street])\n",
        "        \n",
        "\n",
        "    def call(self, flow_vel, a, past_cov, fut_cov):\n",
        "\n",
        "        # sparse matrix\n",
        "        sparse_a = tf.sparse.from_dense(a)\n",
        "  \n",
        "        # flow\n",
        "        flow_0 = flow_vel[:, :, :-self.street]\n",
        "        # velocity\n",
        "        vel_0  = flow_vel[:, :, -self.street:]\n",
        "        \n",
        "\n",
        "        # shape for gcn\n",
        "        flow_sh = tf.reshape(flow_0 , [flow_0.shape[0], flow_0 .shape[2], flow_0.shape[1]])\n",
        "        vel_sh = tf.reshape(vel_0 , [vel_0.shape[0], vel_0.shape[2], vel_0.shape[1]])\n",
        "\n",
        "        # two gcn on flow and vel\n",
        "        gcn_flow = self.gcn_flow_0_enc([flow_sh, sparse_a])\n",
        "        gcn_flow = self.gcn_flow_1_enc([gcn_flow, sparse_a])\n",
        "\n",
        "        gcn_vel = self.gcn_vel_0_enc([vel_sh, sparse_a])\n",
        "        gcn_vel = self.gcn_vel_1_enc([gcn_vel, sparse_a])\n",
        "\n",
        "        # shape for lstm\n",
        "        flow_sh = tf.reshape(gcn_flow, [gcn_flow.shape[0], gcn_flow.shape[2], gcn_flow.shape[1]])\n",
        "        vel_sh = tf.reshape(gcn_vel, [gcn_vel.shape[0], gcn_vel.shape[2], gcn_vel.shape[1]])\n",
        "\n",
        "\n",
        "        # two lstm models\n",
        "        flow_init, h_flow = self.lstm_flow_init(flow_sh)\n",
        "        vel_init, h_vel = self.lstm_vel_init(vel_sh)\n",
        "\n",
        "\n",
        "        # merge layer\n",
        "        # output\n",
        "        flow_vel_i = tf.concat([flow_init, vel_init], axis=2) # \n",
        "\n",
        "        flow_vel = self.dense_temporal_0(flow_vel_i)\n",
        "        flow_vel = self.dense_temporal_1(flow_vel)\n",
        "        flow_vel_f = self.dense_temporal_2(flow_vel)\n",
        "\n",
        "\n",
        "        # multi-head attention\n",
        "        # # Q: flow & vel\n",
        "        # # K: flow\n",
        "        att_flow = self.attention_flow(flow_vel_f, flow_init)\n",
        "\n",
        "\n",
        "        # # Q: vel & vel\n",
        "        # # K: vel\n",
        "        att_vel = self.attention_vel(flow_vel_f, vel_init)\n",
        "\n",
        "        # add\n",
        "        add_flow = self.add([att_flow, flow_vel_f]) # flow_vel, flow_init\n",
        "        add_vel = self.add([att_vel, flow_vel_f]) # vel_init\n",
        "\n",
        "# -----\n",
        "\n",
        "        # concatenate covariates\n",
        "        # output flow_vel - covariates\n",
        "        fut_cov = tf.cast(fut_cov, dtype=tf.float32)\n",
        "        concat_flow = tf.concat([add_flow, fut_cov], axis=2)\n",
        "        concat_vel = tf.concat([add_vel, fut_cov], axis=2)\n",
        "  \n",
        "        # two models for flow and speed respectively\n",
        "\n",
        "        # flow\n",
        "        flow_final = self.lstm_flow_fin(concat_flow , initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) #  flow_vel_f, h_flow flow_vel_f[:,-1,:]\n",
        "        flow = self.drop_flow(flow_final)\n",
        "        flow = self.dense_flow_fin(flow)\n",
        "        flow = self.reshape(flow)\n",
        "\n",
        "\n",
        "        # velocity\n",
        "        vel_final = self.lstm_vel_fin(concat_vel, initial_state = [flow_vel_f[:,-1,:]]) # , initial_state = [flow_vel_f]) \n",
        "        vel = self.drop_vel(vel_final)\n",
        "        vel = self.dense_vel_fin(vel)\n",
        "        vel = self.reshape(vel)\n",
        "\n",
        "\n",
        "        # concatenate two finals results\n",
        "        final = tf.concat([flow, vel], axis=-1)\n",
        "\n",
        "        return final\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = GCN_Net()\n",
        "optimizer = Adam(lr=0.001)\n",
        "loss_fn = tf.keras.losses.MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuLnUrpdmzz"
      },
      "source": [
        "adj_matrix = gcn_filter(adj_matrix, symmetric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vmpXbkO3Uuz",
        "outputId": "0f0f78c3-3a01-498c-cf40-4f96ffbcfc1a"
      },
      "source": [
        "del data_tr, data_te, data_flow, data_vel, data, scaled_tr, scaled_te\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ua6TRjshP3"
      },
      "source": [
        "# Training function\n",
        "@tf.function\n",
        "def train_on_batch(inputs, target, past_cov, fut_cov):\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        predictions= model(inputs, adj_matrix, past_cov, fut_cov, training=True) #predictions\n",
        "        \n",
        "        loss = loss_fn(target, predictions)\n",
        "\n",
        "\n",
        "    variables = model.trainable_variables \n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDbqw5X4shP3",
        "outputId": "9005f91f-8f59-431a-ca65-e74a23bc4dfe"
      },
      "source": [
        "EPOCHS = 250\n",
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "\n",
        "samples_cov = list(loader_tr_cov)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "     \n",
        "    step = 0\n",
        "\n",
        "    for batch in loader_tr:\n",
        "\n",
        "        cov = samples_cov[step]\n",
        "        past_cov = cov[0]\n",
        "        fut_cov = cov[1]\n",
        "        \n",
        "        # Training step\n",
        "        inputs, target = batch\n",
        "        \n",
        "        loss = train_on_batch(inputs, target, past_cov, fut_cov)\n",
        "        \n",
        "        # Track progress\n",
        "        epoch_loss_avg.update_state(loss)\n",
        "\n",
        "        step+=1\n",
        "\n",
        "    # End epoch\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "            print(\"Epoch {}: Loss MAE: {:.5f}\".format(epoch, epoch_loss_avg.result()))\n",
        "        \n",
        "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss MAE: 0.05850\n",
            "Epoch 10: Loss MAE: 0.04613\n",
            "Epoch 20: Loss MAE: 0.04520\n",
            "Epoch 30: Loss MAE: 0.04485\n",
            "Epoch 40: Loss MAE: 0.04481\n",
            "Epoch 50: Loss MAE: 0.04458\n",
            "Epoch 60: Loss MAE: 0.04429\n",
            "Epoch 70: Loss MAE: 0.04418\n",
            "Epoch 80: Loss MAE: 0.04415\n",
            "Epoch 90: Loss MAE: 0.04404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igL4S8uAshP4"
      },
      "source": [
        "fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes.set_ylabel(\"Loss (MAE)\", fontsize=14)\n",
        "axes.plot(train_loss_results)\n",
        "axes.set_xlabel(\"Epoch\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sPebFR-shP4"
      },
      "source": [
        "def inverse_transform(forecasts, scaler):\n",
        "    # invert scaling\n",
        "    inv_pred = scaler.inverse_transform(forecasts)\n",
        "    return inv_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHksYSD3GCkR"
      },
      "source": [
        "def evaluate_forecasts(targets, forecasts, n_seq):\n",
        "    \n",
        "    list_rmse = []\n",
        "    list_mae = []\n",
        "    \n",
        "    for i in range(n_seq):\n",
        "        true = np.vstack([target[i] for target in targets])\n",
        "        predicted = np.vstack([forecast[i] for forecast in forecasts])\n",
        "        \n",
        "        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n",
        "        mae = np.absolute(true - predicted).mean(axis=0)\n",
        "        \n",
        "        list_rmse.append(rmse)\n",
        "        list_mae.append(mae)\n",
        "        \n",
        "    list_rmse = np.vstack(list_rmse)\n",
        "    list_mae = np.vstack(list_mae)\n",
        "    \n",
        "    return list_rmse, list_mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VfOJ2z9shP4"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "forecasts = []\n",
        "targets = []\n",
        "\n",
        "rmse_list = []\n",
        "mae_list = []\n",
        "\n",
        "samples_cov = list(loader_te_cov)\n",
        "\n",
        "\n",
        "del loader_te_cov, loader_tr, loader_tr_cov\n",
        "gc.collect()\n",
        "\n",
        "    \n",
        "for (step, (inp, targ)) in enumerate(tqdm(loader_te)):\n",
        "    \n",
        "            \n",
        "        timestamp = print_datetime[step]\n",
        "\n",
        "        cov = samples_cov[step]\n",
        "        past_cov = cov[0]\n",
        "        fut_cov = cov[1]\n",
        "        \n",
        "        pred  = model(inp, adj_matrix, past_cov, fut_cov, training=False)\n",
        "\n",
        "        truth = inverse_transform(targ[0],  scaler)\n",
        "        pred = inverse_transform(pred[0],  scaler)\n",
        "        \n",
        "        forecasts.append(pred)\n",
        "        targets.append(truth)\n",
        "\n",
        "        rmse, mae = evaluate_forecasts(targets, forecasts, 12)\n",
        "           \n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "\n",
        "        del pred, truth, inp, targ, fut_cov, past_cov, cov\n",
        "        gc.collect()\n",
        "\n",
        "         \n",
        "        fig = plt.figure(figsize=(10, 5))   \n",
        "        plt.plot(np.sum(pred[:, :2874], axis=1), label='Prediction') \n",
        "        plt.plot(np.sum(truth[:, :2874], axis=1), label='Truth') \n",
        "        plt.title('Sum flow Prediction on all highways in Belgium')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        fig.clear()\n",
        "        plt.close(fig)       \n",
        "        plt.show()\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 5))   \n",
        "        plt.plot(np.sum(pred[:, 4524:], axis=1), label='Prediction') \n",
        "        plt.plot(np.sum(truth[:, 4524:], axis=1), label='Truth') \n",
        "        plt.title('Sum Velocity Prediction on all highways in Belgium')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        fig.clear()\n",
        "        plt.close(fig)       \n",
        "        plt.show()\n",
        "\n",
        "        print('* Time step '+str(step))\n",
        "        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n",
        "        print('----')\n",
        "        \n",
        "        \n",
        "        new_instance = scaled_te[step,:].reshape(1,-1)\n",
        "        new_instance_cov = scaled_te_cov[step,:].reshape(1,-1)\n",
        "    \n",
        "        scaled_tr = np.vstack([scaled_tr[1:], new_instance])\n",
        "        scaled_tr_cov = np.vstack([scaled_tr_cov[1:], new_instance_cov])\n",
        "        \n",
        "        loader_new = prepare_data_DL(12, scaled_tr, batch_train)\n",
        "        loader_new_cov = prepare_data_DL(12, scaled_tr_cov, batch_train)\n",
        "        \n",
        "        UPDATE = 2\n",
        "        time_update = 168*granularity\n",
        "\n",
        "        if step % time_update == 0:\n",
        "\n",
        "          print('')\n",
        "          print('* Time to UPDATE the Model after '+str(time_update)+' steps')\n",
        "          print('')\n",
        "\n",
        "          sample_cov_new = list(loader_new_cov) \n",
        "          \n",
        "          for epoch in range(UPDATE):\n",
        "\n",
        "            step_new = 0\n",
        "            \n",
        "            for batch in loader_new:\n",
        "\n",
        "              cov_new = sample_cov_new[step_new]\n",
        "\n",
        "              past_cov_new = cov_new[0]\n",
        "              future_cov_new = cov_new[1]\n",
        "              \n",
        "              inp_new, targ_new = batch\n",
        "\n",
        "              loss = train_on_batch(inp_new, targ_new, past_cov_new, future_cov_new)\n",
        "\n",
        "              # Track progress\n",
        "              epoch_loss_avg.update_state(loss)\n",
        "\n",
        "              # End epoch\n",
        "              train_loss_results.append(epoch_loss_avg.result())\n",
        "\n",
        "              step_new+=1\n",
        "\n",
        "            \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcbFoft2A95Q"
      },
      "source": [
        "np.mean(rmse_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB4n8CvNBF-n"
      },
      "source": [
        "np.mean(mae_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEWDqYTZGNnP"
      },
      "source": [
        "RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\n",
        "RMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n",
        "\n",
        "for i in range(len(RMSE_MEAN)):\n",
        "    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMgrD1upGQ2o"
      },
      "source": [
        "MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\n",
        "MAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n",
        "\n",
        "for i in range(len(MAE_MEAN)):\n",
        "    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J86TlWyE1W9J"
      },
      "source": [
        "with open('MTL_GCN_GRU_RMSE.pkl', 'wb') as f:  \n",
        "    pickle.dump(rmse_list, f)\n",
        "\n",
        "!cp MTL_GCN_GRU_RMSE.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n",
        "print('RMSE')\n",
        "\n",
        "\n",
        "with open('MTL_GCN_GRU_MAE.pkl', 'wb') as f:  \n",
        "    pickle.dump(mae_list, f)\n",
        "\n",
        "!cp MTL_GCN_GRU_MAE.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n",
        "print('MAE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL4u98LoKOu8"
      },
      "source": [
        "with open('MTL_training.pkl', 'wb') as f:  \n",
        "    pickle.dump(train_loss_results, f)\n",
        "\n",
        "!cp MTL_training.pkl \"drive/MyDrive/Colab Notebooks/MTL traffic forecasting/Results/\"\n",
        "print('Training Process')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocz_E41x9wHC"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}