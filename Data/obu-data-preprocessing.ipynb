{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Genarl Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom pytz import timezone\nfrom dateutil import tz\nfrom datetime import datetime, timedelta\nimport geojson\nimport geopandas as gpd  \nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport osmnx as ox\n\n\nimport os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n#pd.options.display.mpl_style = 'default'\nfrom datetime import datetime\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nfrom dateutil import tz\nimport geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom geopandas.tools import sjoin\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.cm as cm\n\nimport folium","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getDuplicateColumns(df):\n    '''\n    Get a list of duplicate columns.\n    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n    :param df: Dataframe object\n    :return: List of columns whose contents are duplicates.\n    '''\n    duplicateColumnNames = set()\n    # Iterate over all the columns in dataframe\n    for x in range(df.shape[1]):\n        # Select column at xth index.\n        col = df.iloc[:, x]\n        # Iterate over all the columns in DataFrame from (x+1)th index till end\n        for y in range(x + 1, df.shape[1]):\n            # Select column at yth index.\n            otherCol = df.iloc[:, y]\n            # Check if two columns at x 7 y index are equal\n            if col.equals(otherCol):\n                duplicateColumnNames.add(df.columns.values[y])\n \n    return list(duplicateColumnNames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anderlecht Streets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bruxelles = gpd.read_file('../input/belgium-obu/Anderlecht_streets.json')\nprint('Anderlecht total number of streets '+str(df_bruxelles.shape[0]))\n\n\npolygons = df_bruxelles\nm = folium.Map([50.85045, 4.34878], zoom_start=13, tiles='cartodbpositron')\nfolium.GeoJson(polygons).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF_5 = pd.read_csv('../input/belgium-obu/Anderlecht_5.csv', header=None)\nDF_5.columns = ['datetime','street_id','count','vel']\nnRow_5, nCol_5 = DF_5.shape\n\nDF_15 = pd.read_csv('../input/belgium-obu/Anderlecht_15.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('../input/belgium-obu/Anderlecht_30.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\nDF_60 = pd.read_csv('../input/belgium-obu/Anderlecht_60.csv', header=None)\nDF_60.columns = ['datetime','street_id','count','vel']\nnRow_60, nCol_60 = DF_60.shape\n\nprint(f'in Anderlecht 5 min there are {nRow_5} rows and {nCol_5} columns')\nprint(f'in Anderlecht 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in Anderlecht 30 min there are {nRow_30} rows and {nCol_30} columns')\nprint(f'in Anderlecht 60 min there are {nRow_60} rows and {nCol_60} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_5 = DF_5.pivot_table(index='datetime', columns='street_id')['count']\ntable_5 = table_5.fillna(0)\n\ntable_vel_5 = DF_5.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_5 = table_vel_5.fillna(0)\n\nprint(table_5.shape)\nprint('')\n\ntable_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\nprint(table_15.shape)\nprint('')\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\nprint(table_30.shape)\nprint('')\n\ntable_60 = DF_60.pivot_table(index='datetime', columns='street_id')['count']\ntable_60 = table_60.fillna(0)\n\ntable_vel_60 = DF_60.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_60 = table_vel_60.fillna(0)\n\nprint(table_60.shape)\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_duplicates = getDuplicateColumns(table_vel_60)\nprint(len(list_duplicates))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = 'Flow_Anderlecht_street_5min'\nprint(file_name)\ntable_5 = table_5.reset_index()#.drop(list_duplicates, axis=1)\ntable_5.to_csv(file_name + '.csv',index=False)\nprint(table_5.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_5min'\nprint(file_name)\ntable_vel_5 = table_vel_5.reset_index()#.drop(list_duplicates, axis=1)\ntable_vel_5.to_csv(file_name + '.csv',index=False)\nprint(table_vel_5.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_Anderlecht_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index()#.drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index()#.drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_Anderlecht_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index()#.drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index()#.drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_Anderlecht_street_60min'\nprint(file_name)\ntable_60 = table_60.reset_index()#.drop(list_duplicates, axis=1)\ntable_60.to_csv(file_name + '.csv',index=False)\nprint(table_60.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_60min'\nprint(file_name)\ntable_vel_60 = table_vel_60.reset_index()#.drop(list_duplicates, axis=1)\ntable_vel_60.to_csv(file_name + '.csv',index=False)\nprint(table_vel_60.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Bruxelles Streets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bruxelles = gpd.read_file('../input/belgium-obu/Bruxelles_streets.json')\nprint('Bruxelles total number of streets '+str(df_bruxelles.shape[0]))\n\n\npolygons = df_bruxelles\nm = folium.Map([50.85045, 4.34878], zoom_start=12, tiles='cartodbpositron')\nfolium.GeoJson(polygons).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## import data 15 min, 30 min, 60 min"},{"metadata":{"trusted":true},"cell_type":"code","source":"DF_15 = pd.read_csv('../input/belgium-obu/Bxl_15.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('../input/belgium-obu/Bxl_30.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\nDF_60 = pd.read_csv('../input/belgium-obu/Bxl_60.csv', header=None)\nDF_60.columns = ['datetime','street_id','count','vel']\nnRow_60, nCol_60 = DF_60.shape\n\n\nprint(f'in BXL 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in BXL 30 min there are {nRow_30} rows and {nCol_30} columns')\nprint(f'in BXL 60 min there are {nRow_60} rows and {nCol_60} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\nprint(table_15.shape)\nprint('')\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\nprint(table_30.shape)\nprint('')\n\ntable_60 = DF_60.pivot_table(index='datetime', columns='street_id')['count']\ntable_60 = table_60.fillna(0)\n\ntable_vel_60 = DF_60.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_60 = table_vel_60.fillna(0)\n\nprint(table_60.shape)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_duplicates = getDuplicateColumns(table_vel_60)\nprint(len(list_duplicates))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = 'Flow_BXL_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index().drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index().drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BXL_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index().drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index().drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BXL_street_60min'\nprint(file_name)\ntable_60 = table_60.reset_index().drop(list_duplicates, axis=1)\ntable_60.to_csv(file_name + '.csv',index=False)\nprint(table_60.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_60min'\nprint(file_name)\ntable_vel_60 = table_vel_60.reset_index().drop(list_duplicates, axis=1)\ntable_vel_60.to_csv(file_name + '.csv',index=False)\nprint(table_vel_60.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Belgium Streets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_belgium = gpd.read_file('../input/belgium-obu/Belgium_streets.json')\nprint('Belgium total number of highways '+str(df_belgium.shape[0]))\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF_15 = pd.read_csv('../input/belgium-obu/Bel_15.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('../input/belgium-obu/Bel_30.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\nDF_60 = pd.read_csv('../input/belgium-obu/Bel_60.csv', header=None)\nDF_60.columns = ['datetime','street_id','count','vel']\nnRow_60, nCol_60 = DF_60.shape\n\nprint(f'in BEL 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in BEL 30 min there are {nRow_30} rows and {nCol_30} columns')\nprint(f'in BEL 60 min there are {nRow_60} rows and {nCol_60} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\ntable_60 = DF_60.pivot_table(index='datetime', columns='street_id')['count']\ntable_60 = table_60.fillna(0)\n\ntable_vel_60 = DF_60.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_60 = table_vel_60.fillna(0)\n\nprint(table_60.shape)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATAFRAME = table_vel_60\nlist_duplicates = getDuplicateColumns(DATAFRAME)\nprint(len(list_duplicates))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = 'Flow_BEL_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index().drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index().drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BEL_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index().drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index().drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BEL_street_60min'\nprint(file_name)\ntable_60 = table_60.reset_index().drop(list_duplicates, axis=1)\ntable_60.to_csv(file_name + '.csv',index=False)\nprint(table_60.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_60min'\nprint(file_name)\ntable_vel_60 = table_vel_60.reset_index().drop(list_duplicates, axis=1)\ntable_vel_60.to_csv(file_name + '.csv',index=False)\nprint(table_vel_60.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To start traffic Predictions check notebooks:\n\n#### * LSTM encoder decoder (Best) - https://www.kaggle.com/giobbu/lstm-encoder-decoder-tensorflow ;\n#### * simple LSTM (Worse) - https://www.kaggle.com/giobbu/simple-lstm-tensorflow ;\n#### * seasonal persistence model (Baseline) - https://www.kaggle.com/giobbu/seasonal-persistence-model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}